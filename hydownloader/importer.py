#!/usr/bin/env python3

# hydownloader
# Copyright (C) 2021  thatfuckingbird

# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.

# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.

import json
import os
import os.path
import sys
import re
import time
import hashlib
from collections import defaultdict
from typing import Optional, Union
import click
import hydrus
from hydownloader import db, log

@click.group()
def cli() -> None:
    pass

def printerr(msg: Union[str, Exception]) -> None:
    print(msg, file=sys.stderr)

def parse_additional_data(result: defaultdict[str, list[str]], data_str: str) -> None:
    """
    This parses the data field of the additonal_data table.
    Currently there are 3 supported formats:
        * empty
        * comma-separated list of tags
        * JSON-object generated by Hydrus Companion,
          format according to the Hydrus API /add_urls/add_url endpoint input format
    """
    if not data_str:
        return
    simple_string = True
    if data_str.strip().startswith('{'):
        d = {}
        try:
            d = json.loads(data_str)
            simple_string = False
        except json.decoder.JSONDecodeError:
            pass
        if "url" in d:
            result["urls"].append(str(d.get("url")))
        s_to_t = d.get("service_names_to_tags", {})
        for key in s_to_t:
            result[key].extend(s_to_t[key])
    if simple_string:
        result[""].extend(x.strip() for x in data_str.split(',') if x.strip())

@cli.command(help='List, rename or delete already imported files')
@click.option('--path', type=str, required=True, help='hydownloader database path.')
@click.option('--action', type=str, required=True, help='The action to perform. One of list, rename, delete.')
@click.option('--do-it', type=bool, is_flag=True, default=False, show_default=True, help='Actually do the renaming or deleting. Off by default.')
@click.option('--no-skip-on-differing-times', type=bool, is_flag=True, default=False, show_default=True, help='Do not skip files that have different creation or modification date than what is in the database of already imported files.')
@click.option('--no-include-metadata', type=bool, is_flag=True, default=False, show_default=True, help='Do not include metadata files in the action.')
def clear_imported(path: str, action: str, do_it: bool, no_skip_on_differing_times: bool, no_include_metadata: bool):
    if action not in ["list", "rename", "delete"]:
        log.fatal("hydownloader-importer", f"Invalid action: {action}")

    log.init(path, True)
    db.init(path)

    log.info("hydownloader-importer", f"Collecting files for action: {action}")

    collected_files : list[tuple[str, str]] = []

    data_path = db.get_datapath()
    for root, _, files in os.walk(data_path):
        for fname in files:
            # json files hold metadata, don't import them to Hydrus
            if fname.endswith('.json'):
                continue

            # already imported file
            if fname.endswith('.HYDL-IMPORTED'):
                continue

            abspath = root + "/" + fname
            path = os.path.relpath(abspath, start = data_path)
            ctime = os.stat(abspath).st_ctime
            mtime = os.stat(abspath).st_mtime

            is_file_in_import_db, db_mtime, db_ctime = db.check_import_db(path)
            if not is_file_in_import_db: continue

            if ctime != db_ctime or mtime != db_mtime:
                if not no_skip_on_differing_times:
                    continue

            # find the path of the associated json metadata file, check if it exists
            # for pixiv ugoira, the same metadata file belongs both to the .webm and the .zip,
            # so this needs special handling
            json_path = abspath+'.json'
            if not os.path.isfile(json_path) and abspath.endswith('.webm'):
                json_path = abspath[:-4]+"zip.json"
            json_exists = os.path.isfile(json_path)
            json_relpath = os.path.relpath(json_path, start = data_path)

            collected_files.append((path,abspath))
            if json_exists and not no_include_metadata:
                collected_files.append((json_relpath,json_path))

    log.info("hydownloader-importer", f"Executing action: {action}")
    for relpath, abspath in collected_files:
        if action == "list":
            print(relpath)
        elif action == "delete":
            print(f"Deleting {abspath}")
            if do_it: os.remove(abspath)
        elif action == "rename":
            print(f"Renaming {abspath}")
            if do_it: os.rename(abspath, abspath+".HYDL-IMPORTED")

@cli.command(help='Run an import job to transfer files and metadata into Hydrus.')
@click.option('--path', type=str, required=True, help='hydownloader database path.')
@click.option('--job', type=str, required=True, help='Name of the import job to run.')
@click.option('--skip-already-imported', type=bool, required=False, default=False, is_flag=True, show_default=True, help='Skip files that were already imported in the past (based on the database of imported files).')
@click.option('--no-skip-on-differing-times', type=bool, required=False, default=False, is_flag=True, show_default=True, help='Do not skip files that have different creation or modification date than what is in the database of already imported files. This flag only has effect when --skip-already-imported is used.')
@click.option('--config', type=str, required=False, default=None, show_default=True, help='Import job configuration filepath override.')
@click.option('--verbose', type=bool, is_flag=True, default=False, show_default=True, help='Print generated metadata and other information.')
@click.option('--do-it', type=bool, is_flag=True, default=False, show_default=True, help='Actually do the importing. Off by default.')
@click.option('--no-abort-on-missing-metadata', type=bool, is_flag=True, show_default=True, default=False, help='Do not stop importing when a metadata file is not found.')
@click.option('--filename-regex', type=str, default=None, show_default=True, help='Only run the importer on files whose filepath matches the regex given here. This is an additional restriction on top of the filters defined in the import job.')
@click.option('--no-abort-on-error', type=bool, default=False, show_default=True, is_flag=True, help='Do not abort on errors. Useful to check for any potential errors before actually importing files.')
def run_job(path: str, job: str, skip_already_imported: bool, no_skip_on_differing_times: bool, config: Optional[str], verbose: bool, do_it: bool, no_abort_on_missing_metadata: bool, filename_regex: Optional[str], no_abort_on_errors: bool) -> None:
    log.init(path, True)
    db.init(path)

    config_path = db.get_rootpath()+'/hydownloader-import-jobs.json'
    data_path = db.get_datapath()
    if config:
        config_path = config
    if not os.path.isfile(config_path):
        log.fatal("hydownloader-importer", f"Configuration file not found: {config_path}")

    jobs = json.load(open(config_path, 'r', encoding='utf-8-sig'))
    if not job in jobs:
        log.fatal("hydownloader-importer", f"Job not found in configuration file: {job}")
    jd = jobs[job]

    force_add_metadata = jd.get('forceAddMetadata', True)
    force_add_files = jd.get('forceAddFiles', False)
    order_folder_contents = jd.get('orderFolderContents', 'default')

    client = hydrus.Client(jd['apiKey'], jd['apiURL'])

    log.info("hydownloader-importer", f"Starting import job: {job}")

    # iterate over all files in the data directory
    for root, _, files in os.walk(data_path):
        # sort files before iterating over them
        if order_folder_contents == "name":
            files = sorted(files)
        elif order_folder_contents == "mtime":
            files = sorted(files, key=lambda t: os.stat(t).st_mtime)
        elif order_folder_contents == "ctime":
            files = sorted(files, key=lambda t: os.stat(t).st_ctime)
        elif order_folder_contents != "default":
            printerr("The value of the orderFolderContents option is invalid")
            sys.exit(1)

        for fname in files:
            # json files hold metadata, don't import them to Hydrus
            if fname.endswith('.json'):
                continue

            # already imported file
            if fname.endswith('.HYDL-IMPORTED'):
                continue

            abspath = root + "/" + fname
            path = os.path.relpath(abspath, start = data_path)
            if filename_regex and not re.match(filename_regex, path):
                continue

            # set up some variables
            # some will be used later in the code, some are meant to be used in user-defined expressions
            ctime = os.stat(abspath).st_ctime
            mtime = os.stat(abspath).st_mtime
            split_path = os.path.split(path)
            fname_noext, fname_ext = os.path.splitext(fname)
            if fname_ext.startswith('.'): fname_ext = fname_ext[1:]

            # find the path of the associated json metadata file, check if it exists
            # for pixiv ugoira, the same metadata file belongs both to the .webm and the .zip,
            # so this needs special handling
            json_path = abspath+'.json'
            if not os.path.isfile(json_path) and abspath.endswith('.webm'):
                json_path = abspath[:-4]+"zip.json"
            json_exists = True
            raw_metadata = None
            if not os.path.isfile(json_path):
                json_exists = False
                printerr(f"Warning: no metadata file found for {path}")
                if not no_abort_on_missing_metadata or not no_abort_on_errors:
                    sys.exit(1)
            else:
                raw_metadata = open(json_path, "rb").read()

            generated_urls = set()
            generated_tags : set[tuple[str, str]] = set()
            matched = False # will be true if at least 1 filter group matched the file
            json_data = None # this will hold the associated json metadata (if available)

            if verbose: printerr(f"Processing file: {path}...")

            # iterate over all filter groups, do they match this file?
            for group in jd['groups']:
                # evaluate filter, load json metadata if the filter matches and we haven't loaded it yet
                should_process = False
                metadata_only = group.get("metadataOnly", False)
                try:
                    should_process = eval(group['filter'])
                except:
                    printerr(f"Failed to evaluate filter: {group['filter']}")
                    if not no_abort_on_errors: sys.exit(1)
                if not json_data and json_exists:
                    try:
                        json_data = json.load(open(json_path,encoding='utf-8-sig'))
                    except json.decoder.JSONDecodeError:
                        printerr(f"Failed to parse JSON: {json_path}")
                        if not no_abort_on_errors: sys.exit(1)
                if not should_process:
                    continue
                if not metadata_only:
                    matched = True

                # get the data for this file from the additional_data db table and process it
                # set up some variables that user-defined expressions will be able to use
                additional_data_dicts = db.get_additional_data_for_file(path)
                if not additional_data_dicts and path.endswith('.webm'):
                    additional_data_dicts = db.get_additional_data_for_file(path[:-4]+"zip")
                extra_tags : defaultdict[str, list[str]] = defaultdict(list)
                min_time_added = -1
                max_time_added = -1
                for d in additional_data_dicts:
                    parse_additional_data(extra_tags, d['data'])
                    if min_time_added == -1 or min_time_added > d['time_added']:
                        min_time_added = d['time_added']
                    if max_time_added == -1 or max_time_added < d['time_added']:
                        max_time_added = d['time_added']
                sub_ids = []
                url_ids = []
                for d in additional_data_dicts:
                    if d['subscription_id']:
                        sub_ids.append(str(d['subscription_id']))
                    if d['url_id']:
                        url_ids.append(str(d['url_id']))

                # execute user-defined tag and url generator expressions
                has_error = False
                for dtype, d in [('tag',x) for x in group.get('tags', [])]+[('url',x) for x in group.get('urls', [])]:
                    skip_on_error = d.get("skipOnError", False)
                    allow_empty = d.get("allowEmpty", False)
                    rule_name = d.get("name")
                    generated_results = []
                    # if the expression is a single string
                    if isinstance(d["values"], str):
                        try:
                            eval_res = eval(d["values"])
                            # check result type: must be string or iterable of strings
                            if isinstance(eval_res, str):
                                generated_results = [eval_res]
                            else:
                                for eval_res_str in eval_res:
                                    if not isinstance(eval_res_str, str):
                                        printerr(f"Invalid result type ({str(type(eval_res_str))}) while evaluating expression: {d['values']}")
                                        if not no_abort_on_errors: sys.exit(1)
                                    else:
                                        generated_results.append(eval_res_str)
                        except Exception as e:
                            if verbose:
                                printerr(f"Failed to evaluate expression: {d['values']}")
                                print(e)
                            has_error = True
                    else: # multiple expressions (array of strings)
                        for eval_expr in d["values"]:
                            try:
                                eval_res = eval(eval_expr)
                                # check result type: must be string or iterable of strings
                                if isinstance(eval_res, str):
                                    generated_results = [eval_res]
                                else:
                                    for eval_res_str in eval_res:
                                        if not isinstance(eval_res_str, str):
                                            printerr(f"Invalid result type ({str(type(eval_res_str))}) while evaluating expression: {eval_expr}")
                                            if not no_abort_on_errors: sys.exit(1)
                                        else:
                                            generated_results.append(eval_res_str)
                            except Exception as e:
                                if verbose:
                                    printerr(f"Failed to evaluate expression: {eval_expr}")
                                    printerr(e)
                                has_error = True

                    # check for empty results or failed evaluation, as necessary
                    if not generated_results and not allow_empty:
                        printerr(f"Error: the rule named {rule_name} yielded no results but this is not allowed")
                        if not no_abort_on_errors: sys.exit(1)
                    if has_error:
                        printerr(f"Warning: an expression failed to evaluate in the rule named {rule_name}")
                        if not skip_on_error:
                            if not no_abort_on_errors: sys.exit(1)

                    # save results of the currently evaluated expressions
                    if dtype == 'url':
                        generated_urls.update(generated_results)
                    else:
                        if "tagRepos" in d: # predefined tag repos
                            for repo in d["tagRepos"]:
                                generated_tags.update((repo,tag) for tag in generated_results)
                        else: # tag repos should be extracted from the tags
                            for tag in generated_results:
                                if not ":" in tag:
                                    printerr(f"The generated tag '{tag}' must start with a tag repo name. In rule: {rule_name}.")
                                    if not no_abort_on_errors: sys.exit(1)
                                else:
                                    repo = tag.split(":")[0]
                                    actual_tag = ":".join(tag.split(":")[1:])
                                    generated_tags.add((repo,actual_tag))
            if matched:
                printerr(f"File matched: {path}...")

                if not os.path.getsize(abspath):
                    print(f"Found truncated file, won't be imported: {abspath}")
                    if not no_abort_on_errors:
                        sys.exit(1)
                    else:
                        continue

                if verbose:
                    printerr("Generated URLs:")
                    for url in generated_urls:
                        printerr(url)
                    printerr("Generated tags:")
                    for repo, tag in sorted(list(generated_tags), key=lambda x: x[0]):
                        printerr(f"{repo} <- {tag}")
                if verbose: printerr('Hashing...')

                is_file_in_import_db, db_mtime, db_ctime = db.check_import_db(path)
                if skip_already_imported and is_file_in_import_db:
                    if not (no_skip_on_differing_times and (db_mtime != mtime or db_ctime != ctime)):
                        continue

                # calculate hash, check if Hydrus already knows the file
                already_added = False
                hexdigest = str()
                if do_it:
                    hasher = hashlib.sha256()
                    with open(abspath, 'rb') as hashedfile:
                        buf = hashedfile.read(65536 * 16)
                        while len(buf) > 0:
                            hasher.update(buf)
                            buf = hashedfile.read(65536 * 16)
                    hexdigest = hasher.hexdigest()
                    if client.file_metadata(hashes=[hexdigest], only_identifiers=True):
                        printerr("File is already in Hydrus")
                        already_added = True
                # send file, tags, metadata to Hydrus as needed
                if not already_added or force_add_files:
                    if verbose: printerr("Sending file to Hydrus...")
                    if do_it: client.add_file(abspath)
                if not already_added or force_add_metadata:
                    if verbose: printerr("Associating URLs...")
                    if do_it: client.associate_url(hashes=[hexdigest],add=generated_urls)
                    if verbose: printerr("Adding tags...")
                    tag_dict = defaultdict(list)
                    for repo, tag in generated_tags:
                        tag_dict[repo].append(tag)
                    if do_it:
                        client.add_tags(hashes=[hexdigest],service_to_tags=tag_dict)
                if do_it:
                    if verbose: printerr("Writing entry to import database...")
                    db.add_or_update_import_entry(path, import_time=time.time(), creation_time=ctime, modification_time=mtime, metadata=raw_metadata, hexdigest=hexdigest)

            else:
                if verbose: printerr(f"Skipping due to no matching filter: {path}")

    log.info("hydownloader-importer", f"Finished import job: {job}")
    db.shutdown()

def main() -> None:
    cli()
    ctx = click.get_current_context()
    click.echo(ctx.get_help())
    ctx.exit()

if __name__ == "__main__":
    main()
